GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
# of training data:  431394
# of validation data:  50845

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name       | Type                        | Params | Mode
-------------------------------------------------------------------
0 | potential  | EquiformerV2_OC20           | 13.9 M | train
1 | loss_fn    | L1Loss                      | 0      | train
2 | MAEEval    | MeanAbsoluteError           | 0      | train
3 | MAPEEval   | MeanAbsolutePercentageError | 0      | train
4 | cosineEval | CosineSimilarity            | 0      | train
-------------------------------------------------------------------
13.9 M    Trainable params
0         Non-trainable params
13.9 M    Total params
55.791    Total estimated model params size (MB)
335       Modules in train mode
0         Modules in eval mode
Sanity Checking DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.15it/s]val epoch 0 val-MAE_E 1.83 val-MAE_F 0.13 val-MAE_hessian 7.95 val-totloss 52.51
Epoch 0:   2%|██▉                                                                                                                                                                          | 27/1600 [00:14<14:12,  1.85it/s, v_num=5el0]

Detected KeyboardInterrupt, attempting graceful shutdown ...
Error in sys.excepthook:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/wandb/sdk/lib/exit_hooks.py", line 52, in exc_handler
    traceback.print_exception(exc_type, exc, tb)
  File "/root/miniconda3/lib/python3.10/traceback.py", line 119, in print_exception
    te = TracebackException(type(value), value, tb, limit=limit, compact=True)
  File "/root/miniconda3/lib/python3.10/traceback.py", line 502, in __init__
    self.stack = StackSummary.extract(
  File "/root/miniconda3/lib/python3.10/traceback.py", line 370, in extract
    linecache.lazycache(filename, f.f_globals)
  File "/root/miniconda3/lib/python3.10/linecache.py", line 147, in lazycache
    def lazycache(filename, module_globals):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 427488) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.

Original exception was:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 46, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 574, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 981, in _run
    results = self._run_stage()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1025, in _run_stage
    self.fit_loop.run()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 167, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1306, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py", line 164, in step
    loss = closure()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
    closure_result = closure()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 317, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 319, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 389, in training_step
    return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 640, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1523, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1359, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 633, in wrapped_forward
    out = method(*_args, **_kwargs)
  File "/root/HORM-final/training_module.py", line 312, in training_step
    loss, info = self.compute_loss(batch)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/HORM-final/training_module.py", line 289, in compute_loss
    hat_ae, hat_forces = self.potential.forward(
  File "/root/HORM-final/ocpmodels/common/utils.py", line 132, in cls_method
    return f(self, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/HORM-final/nets/equiformer_v2/equiformer_v2_oc20.py", line 472, in forward
    hessian_ij=self.grad_hess_ij(energy=energy,posj=posj,posi=posi)
  File "/root/HORM-final/nets/equiformer_v2/equiformer_v2_oc20.py", line 363, in grad_hess_ij
    gji = -grad([fj[:, i].sum()], [posi], create_graph=create_graph, retain_graph=True)[0]
  File "/root/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/root/HORM-final/train.py", line 128, in <module>
    trainer.fit(pm)
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 538, in fit
    call._call_and_handle_interrupt(
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 60, in _call_and_handle_interrupt
    trainer._teardown()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1004, in _teardown
    self.strategy.teardown()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 419, in teardown
    super().teardown()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/parallel.py", line 133, in teardown
    super().teardown()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 538, in teardown
    self.accelerator.teardown()
  File "/root/miniconda3/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py", line 82, in teardown
    _clear_cuda_memory()
  File "/root/miniconda3/lib/python3.10/site-packages/lightning_fabric/accelerators/cuda.py", line 181, in _clear_cuda_memory
    torch.cuda.empty_cache()
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/memory.py", line 162, in empty_cache
    torch._C._cuda_emptyCache()
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 417757) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
