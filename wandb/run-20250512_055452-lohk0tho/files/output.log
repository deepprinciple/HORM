GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
# of training data:  431394
# of validation data:  50845

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name       | Type                        | Params | Mode
-------------------------------------------------------------------
0 | potential  | Potential                   | 10.8 M | train
1 | loss_fn    | L1Loss                      | 0      | train
2 | MAEEval    | MeanAbsoluteError           | 0      | train
3 | MAPEEval   | MeanAbsolutePercentageError | 0      | train
4 | cosineEval | CosineSimilarity            | 0      | train
-------------------------------------------------------------------
10.8 M    Trainable params
0         Non-trainable params
10.8 M    Total params
43.200    Total estimated model params size (MB)
375       Modules in train mode
0         Modules in eval mode
Sanity Checking DataLoader 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.47it/s]val epoch 0 val-MAE_E 1.85 val-MAE_F 0.13 val-MAE_hessian 7.69 val-totloss 51.40
/root/HORM-final/leftnet/model/leftnet.py:696: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:63.)
  coord_cross = torch.cross(pos[i], pos[j])
Epoch 0:   1%|█▌                                                                                                                                                                           | 15/1600 [00:03<06:11,  4.27it/s, v_num=0tho]
/root/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 196], strides() = [1, 1]
bucket_view.sizes() = [1, 196], strides() = [196, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

Detected KeyboardInterrupt, attempting graceful shutdown ...
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9c3d0b6dd0>
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1479, in __del__
    self._shutdown_workers()
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1443, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/root/miniconda3/lib/python3.10/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/root/miniconda3/lib/python3.10/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/root/miniconda3/lib/python3.10/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 457407) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.
