_wandb:
    value:
        cli_version: 0.19.8
        m:
            - "1": trainer/global_step
              "6":
                - 3
              "7": []
        python_version: 3.10.14
        t:
            "1":
                - 1
                - 9
                - 55
                - 77
                - 103
            "2":
                - 1
                - 9
                - 55
                - 77
                - 103
            "3":
                - 7
                - 13
                - 23
                - 55
                - 66
            "4": 3.10.14
            "5": 0.19.8
            "8":
                - 5
            "12": 0.19.8
            "13": linux-x86_64
model_config:
    value:
        a: 0.35
        b: 0
        chi2: 8
        compute_forces: true
        compute_stress: false
        cutoff: 5
        device: cuda
        direct_forces: false
        eps: 1e-06
        has_dropout_flag: true
        has_norm_after_flag: false
        has_norm_before_flag: true
        head: 16
        hidden_channels: 128
        hidden_channels_chi: 96
        main_chi1: 32
        mp_chi1: 32
        name: Alphanet
        num_layers: 4
        num_radial: 64
        num_targets: 1
        output_dim: 1
        pos_require_grad: true
        readout: sum
        reduce_mode: sum
        use_pbc: false
        use_sigmoid: false
optimizer_config:
    value:
        amsgrad: true
        betas:
            - 0.9
            - 0.999
        lr: 0.0005
        weight_decay: 0
training_config:
    value:
        bz: 8
        clip_grad: true
        datadir: /deepprinciple-proj-dev/datasets/transition-1x/
        ema: false
        gradient_clip_val: 0.1
        lr_schedule_config:
            gamma: 0.85
            step_size: 50
        lr_schedule_type: step
        num_workers: 48
